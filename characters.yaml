Governments:
  conditions:
    - Impose compute caps and enforce reporting
    - Create empowered AI oversight agencies
    - Negotiate international agreements for verification and enforcement
    - Shift national security framing from AGI race to Tool AI programs
  current_state:
    - Political focus on economic growth and national security competition
    - Oversight agencies limited or nonexistent
    - Export controls exist but no international treaties on AI compute
    - Security establishments frame AGI as strategic advantage, like nuclear weapons
  gaps:
    - severity: Large
      explanation: Governments pursue economic and security competition rather than restraint
    - severity: Large
      explanation: Oversight agencies lack authority and speed; regulation lags industry progress
    - severity: Critical
      explanation: No international treaties or institutions; rivalry (e.g., US–China) intensifies
    - severity: Critical
      explanation: National security establishments view AGI race as winnable, opposite of the plan
  MarkdownContext: |
    ## Governments & Security Establishments
    - **Influence**: Can legislate compute caps, enforce reporting, establish oversight agencies, and negotiate international treaties. Security establishments must shift from AGI “Manhattan projects” to Tool AI “Apollo projects.”
    - **Resources/Capabilities**: Lawmaking power, export controls, intelligence and defense budgets, diplomatic channels.
    - **Motivations**: 
      - Publicly: economic growth, technological leadership, scientific progress.
      - Privately: fear of rivals gaining a decisive strategic advantage; belief that AGI could revolutionize military affairs (comparable to nuclear weapons).
    - **Interactions**: Strongly influenced by corporate lobbying; in rivalry with other governments (e.g., US–China), affecting chipmakers and regulators.

Corporations:
  conditions:
    - Accept strict liability for dangerous systems
    - Comply with compute oversight and hard caps
    - Pivot business models from AGI toward Tool AI
    - Respond to cultural and social pressure to change mission
  current_state:
    - Corporations resist regulation, opposing liability frameworks such as California’s SB1047
    - Currently maximize compute use in pursuit of AGI capabilities
    - Business incentives favor labor-replacing AGI systems
    - Employee dissent exists but leadership remains committed to AGI race
  gaps:
    - severity: Critical
      explanation: Strong resistance to liability threatens public accountability
    - severity: Critical
      explanation: Corporate incentives favor AGI race; no acceptance of compute caps
    - severity: Critical
      explanation: Pivot to Tool AI conflicts with profit logic of automation
    - severity: Large
      explanation: Some cultural dissent, but weak relative to leadership and profit incentives
  MarkdownContext: |
    ## Corporations (Major Technology Companies)
    - **Influence**: Directly control AI development trajectory; could stop the AGI race and redirect toward Tool AI. Possess lobbying power to resist or shape regulation.
    - **Resources/Capabilities**: Massive compute, datasets, top AI researchers, financial power, ability to frame narratives of AGI inevitability.
    - **Motivations**:
      - Enormous financial gains: potential to capture trillions by replacing human labor.
      - Drive to dominate new digital markets across all sectors simultaneously.
      - Executive visions of technological transformation and prestige.
    - **Interactions**: Depend on hardware manufacturers for chips; heavily influence governments and regulators.

HardwareManufacturers:
  conditions:
    - Embed governance features in chips (cryptographic locks, metering, geolocation)
    - Cooperate with government oversight and reporting schemes
    - Support global standards without loopholes
    - Accept reduced sales in exchange for stability and safety
  current_state:
    - Highly concentrated supply chain (TSMC, ASML, NVIDIA, AMD) makes oversight feasible
    - Some export controls in place (e.g., US restrictions to China)
    - No global harmonized standards; unilateral controls create loopholes
    - Firms profit from maximizing demand; no incentives to restrain sales
  gaps:
    - severity: Moderate
      explanation: Technical feasibility proven, but dependent on government mandates
    - severity: Moderate
      explanation: Partial oversight exists, but limited cooperation
    - severity: Large
      explanation: Global standards absent due to geopolitical rivalry
    - severity: Large
      explanation: Commercial incentives oppose restraint; alignment requires regulation
  MarkdownContext: |
    ## Hardware Manufacturers & Chip Supply Chain
    - **Influence**: Can embed governance features into chips, enforce compute metering, and serve as chokepoints for international verification.
    - **Resources/Capabilities**: Concentrated global supply chain (TSMC, ASML, NVIDIA, AMD), technical feasibility to implement cryptographic and security controls.
    - **Motivations**:
      - Profit from soaring chip demand, driven by AI race.
      - Competitive advantage in controlling scarce high-end chip supply.
    - **Interactions**: Depend on government mandates for governance; affected by geopolitical rivalries between governments.

Regulators:
  conditions:
    - Stay independent from corporate and national capture
    - Develop clear technical standards for compute reporting and accounting
    - Create legitimate global governance institutions
  current_state:
    - Industry lobbying strongly influences rulemaking; EU AI Act exempts military AI and does not prohibit AGI
    - Early standards like Frontier Model Forum exist but lack enforcement
    - No global AI governance body exists; only proposals for CERN/IAEA-style institutions
  gaps:
    - severity: Large
      explanation: Regulators heavily influenced by corporate lobbying and national agendas
    - severity: Moderate
      explanation: Groundwork exists but technical standards lack precision and enforcement
    - severity: Critical
      explanation: No international enforcement mechanisms; rival governments do not cooperate
  MarkdownContext: |
    ## Regulators & International Institutions
    - **Influence**: Can classify AGI as unacceptable risk, enforce tiered licensing, and potentially create international bodies like an AI IAEA.
    - **Resources/Capabilities**: Authority to set laws, technical standards, and enforcement regimes; ability to mandate liability.
    - **Motivations**:
      - Protect public safety and prevent systemic risks.
      - Balance innovation with oversight.
      - In practice, often pressured to prioritize economic competitiveness and industry lobbying.
    - **Interactions**: Vulnerable to corporate capture; depend on governments for legitimacy and on international cooperation for enforcement.

CivilSociety:
  conditions:
    - Be well-informed on AGI risks and Tool AI alternatives
    - Build broad coalitions across NGOs, labor, and media
    - Exert democratic leverage early to influence policy
  current_state:
    - Public polls show opposition to AGI but understanding of risks is shallow
    - NGOs exist (Future of Life Institute, alignment groups) but fragmented and weak relative to corporations
    - Institutions weak; lobbying power of corporations outweighs public preference
  gaps:
    - severity: Moderate
      explanation: Public instincts align but education and awareness are lacking
    - severity: Large
      explanation: Coalitions fragmented; need stronger coordination
    - severity: Large
      explanation: Democratic pressure weak compared to corporate lobbying and declining institutional trust
  MarkdownContext: |
    ## Civil Society & the Public
    - **Influence**: Can mobilize public opinion, pressure policymakers, resist AGI inevitability narratives, and demand redistributive mechanisms like data dignity.
    - **Resources/Capabilities**: NGOs, advocacy groups, democratic leverage, public polling already showing opposition to AGI.
    - **Motivations**:
      - Desire for safety, slower AI development, and protection of jobs.
      - Fear of power concentration in corporations or governments.
      - Aspirations for equitable distribution of AI benefits.
    - **Interactions**: Influence governments and regulators through democratic processes; need to build coalitions with scientists and media.

ScientificCommunity:
  conditions:
    - Resist corporate funding capture
    - Coordinate globally to prevent 'safety tourism'
    - Engage public and policymakers with credible advocacy
  current_state:
    - Many researchers absorbed by big tech labs; independent research underfunded
    - Some collaboration among AI safety networks but fragmented
    - Prominent researchers (Hinton, Bengio) actively warn of extinction risks
  gaps:
    - severity: Large
      explanation: Corporate incentives dominate research directions
    - severity: Moderate
      explanation: Global coordination exists but fragmented and underpowered
    - severity: Small
      explanation: Advocacy growing and aligned, but not yet mainstreamed
  MarkdownContext: |
    ## Scientific & AI Research Community
    - **Influence**: Provide technical frameworks for Tool AI and safety verification, educate the public and policymakers, reframe narratives around AI safety.
    - **Resources/Capabilities**: Technical expertise, credibility, ability to innovate safer architectures and red-team unsafe systems.
    - **Motivations**:
      - Curiosity and scientific drive to understand intelligence.
      - Desire to use AI for positive ends (medicine, science, sustainability).
      - Concern over existential risks and responsibility to warn society.
    - **Interactions**: Many researchers work in corporations, creating capture risks; align with NGOs and civil society to advocate for restraint.
