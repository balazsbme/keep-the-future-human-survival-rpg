# Keep the Future Human – Action Plan Summary

## 1. Core Goal
- **Close the gates** to Artificial General Intelligence (AGI) and superintelligence.  
- Prevent the creation of smarter-than-human autonomous systems that would replace human agency and civilization.  
- Redirect AI development toward **powerful but controllable Tool AI** that strengthens human capabilities without disempowering us.  

---

## 2. Key Actions to Prevent AGI & Superintelligence

### 2.1 Governance of Compute (AI Hardware Power)
- **Accounting & Transparency**  
  - Establish standardized reporting of compute usage (training FLOPs and inference FLOPs per second).  
  - Require all developers above thresholds to report usage, eventually with cryptographic verification.  
- **Hard Compute Caps**  
  - Impose strict limits on maximum training compute (e.g., 10^27 FLOPs) and inference rate (e.g., 10^20 FLOP/s).  
  - These limits function as a "gate" preventing runaway intelligence escalation.  
- **Hardware Security Features**  
  - Embed control features in AI-specialized chips:  
    - Geolocation checks,  
    - Allow-listed chip clusters,  
    - Usage metering & automatic shut-offs,  
    - Speed limits,  
    - Cryptographically attested training proofs.  
- **International Enforcement**  
  - Build treaties akin to nuclear non-proliferation.  
  - Verification relies on a handful of companies controlling chip supply chains (TSMC, NVIDIA, AMD, etc.).  

**Key actors that must change:**  
- **Governments**: legislate and enforce compute oversight.  
- **Chipmakers**: integrate mandatory security and reporting features.  
- **Tech companies**: accept binding limits instead of racing toward AGI.  

---

### 2.2 Liability Regime for Dangerous AI
- Treat creation of **high autonomy + high generality + high intelligence systems** as an **“abnormally dangerous activity.”**  
- **Strict, joint-and-several liability** for harms caused by such systems, extending to executives and boards.  
- **Safe harbors** for:  
  - Systems below compute thresholds,  
  - Narrow or passive AI,  
  - Systems proven to be secure and controllable.  
- **Injunctive Relief**: courts empowered to halt risky AI development pre-emptively.  

**Key actors that must change:**  
- **Lawmakers**: pass liability laws covering AGI-like risks.  
- **Courts/regulators**: enforce liability and block unsafe projects.  
- **AI firms**: assume personal accountability, not outsource risk to the public.  

---

### 2.3 Tiered Regulation by Risk
- **Risk Tiers (RT-0 to RT-4):**  
  - *RT-0*: Weak, narrow AI → minimal oversight.  
  - *RT-1/2*: Strong in 1–2 dimensions (autonomy, generality, intelligence) → registration, safety audits.  
  - *RT-3*: Full AGI (triple intersection) → requires strict pre-approval, formal safety guarantees, and non-removable kill-switches.  
  - *RT-4*: Beyond compute caps (superintelligence) → prohibited.  
- **National agencies** (existing or new) empowered to license, audit, and block unsafe models.  
- **International harmonization** to prevent loopholes.  

**Key actors that must change:**  
- **Governments/regulators**: build fast, empowered AI oversight agencies.  
- **Industry**: accept graded regulation, not fight all regulation.  

---

## 3. Redirecting AI Development: What to Build Instead

### 3.1 Tool AI
- Build **AI that empowers humans while staying under control**.  
- Avoid the dangerous triple-intersection of autonomy + generality + intelligence.  
- Examples: medical assistants, scientific research tools, educational tutors, democratic process enhancers.  
- Composite/multi-module systems with human oversight, instead of monolithic black-box AGI.  

### 3.2 Security & Verifiability
- Invest in **formal verification methods** to prove AI has safety properties (e.g., guaranteed shutdown).  
- Treat AI like aviation or pharmaceuticals: safety cases with quantifiable guarantees.  

### 3.3 Economic & Social Design
- New ownership and development models:  
  - Public, non-profit, or cooperatively governed AI labs.  
  - “Data dignity” schemes where humans are compensated for training data contributions.  
  - Sovereign or universal wealth funds from AI profits to distribute benefits broadly.  
- Curb excessive **power concentration** in Big Tech or national security institutions.  

### 3.4 Strengthening Governance & Society
- Use AI to **rebuild weakened institutions**:  
  - Trusted epistemic systems (fact provenance, cryptographic content authentication).  
  - New coordination mechanisms (modeling incentives, facilitating collective action).  
  - Enhanced public discourse (AI intermediaries to bridge divides, “talk to a city” interfaces).  

---

## 4. National Security Strategy Shift
- Stop reckless AGI “Manhattan projects.”  
- Instead pursue an **“Apollo project” for secure, trustworthy Tool AI**, including:  
  - National AI research infrastructure,  
  - Verified secure software ecosystems,  
  - Secure chip supply chains,  
  - AI-driven national defense systems that are **controllable and loyal**, not autonomous AGI.  

---

## 5. Social & Political Will
- The technical tools exist, but **political and social will are missing**.  
- Public majorities already favor slower, safer AI.  
- Need:  
  - Mobilization of citizen pressure,  
  - Education on risks,  
  - Clear vision that prosperity and progress are possible without AGI.  

---

## 6. Endgame Vision
- Humanity should **choose deliberately**, not stumble into building a replacement species.  
- Close the Gates with enforceable limits.  
- Build transformative **Tool AI ecosystems** that:  
  - Empower individuals,  
  - Distribute economic benefits broadly,  
  - Strengthen democracy and global coordination,  
  - Keep the future human.  

---

# Key Changes Required of Actors

| Actor | Required Changes |
|-------|------------------|
| **Governments** | Impose compute caps, regulate AI tiers, enforce liability, build oversight agencies, negotiate international agreements. |
| **Tech Companies** | Halt AGI race, comply with compute oversight, accept liability, pivot business models toward Tool AI. |
| **Chipmakers (NVIDIA, TSMC, ASML, AMD, etc.)** | Embed hardware governance features, cooperate with verification schemes. |
| **National Security Establishments** | Replace AGI arms race with secure Tool AI programs, prioritize verifiability. |
| **Civil Society / Public** | Demand slower AI development, push policymakers to act, reject inevitability narrative. |
| **Academia & NGOs** | Provide frameworks for safe AI governance, develop verified control methods, educate the public. |
